## 强化学习简介

- ### 强化学习基础

  - #### 强化学习和监督学习以及非监督学习的区别

    - 监督学习是使用已经标记好的数据样本，做训练来预测新的数据的类型（分类）或者值（回归）
    - 非监督学习是找到unlabeled 数据背后的关系
    - 而强化学习的目标是最大化最大化累计回报

  - #### 概念

    - 强化学习分为两大类——model-free or model-based
    - Exploring Starts 假设 - 指有一个探索起点的环境。
    - 比如：围棋的当前状态就是一个探索起点。自动驾驶的汽车也许是一个没有起点的例子。
    - first-visit - 在一段情节中，一个状态只会出现一次，或者只需计算第一次的价值。
    - every-visit - 在一段情节中，一个状态可能会被访问多次，需要计算每一次的价值。
    - on-policy method - 评估和优化的策略和模拟的策略是同一个。
      - 存在局部最优问题，使用ε-greedy 解决
    - off-policy method - 评估和优化的策略和模拟的策略是不同的两个。有时候，模拟数据来源于其它处，比如：已有的数据，或者人工模拟等等。
      - target policy - 目标策略。off policy method中，需要优化的策略。
      - behavior policy - 行为策略。off policy method中，模拟数据来源的策略。
      - 优势
        - 更强的通用性，确保了数据全面性，所有行为都能覆盖
        - 保证了探索性，在选取next的时候，需要选取max的一个，覆盖了action 的所有q值，从而覆盖了所有行为
      - 劣势
        - 过高估计问题——解决方法：Double learning --->这在reinforcement learning an introduction 和 DDQN里面都有

  - ### 表格式方法

    - #### 马尔科夫性质

      - 当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态，与过去状态是条件独立的（即与历史路径没有关系）
      - 具有马尔可夫性质的过程通常称之为马尔可夫过程

    - #### 贝尔曼公式

      - 贝尔曼公式

        ![img](https://s2.ax1x.com/2019/02/24/k4Tam6.png)

      - 贝尔曼值最优化方程 (for v*)

        ![img](https://s2.ax1x.com/2019/02/24/k4Typd.md.png)

      - 贝尔曼值最优方程(for q*)

        ![img](https://s2.ax1x.com/2019/02/24/k4T61A.png)

    - #### DP

      - 通用迭代公式(Generalized Policy Iteration)

        - 主要思想：1.从一个策略π0开始，显示策略评估，得到策略π0的价值Vπ0，2.策略改善，根据价值Vπ0，优化策略为π0'，3.重复迭代前两个步骤，得到最优策略π*（终止条件π不变了，或者策略价值不变了）

        - Policy Iteration

          ![img](https://s2.ax1x.com/2019/02/24/k4TgXt.md.png)

        - Value Iteration

          ![img](https://s2.ax1x.com/2019/02/24/k4T5tg.md.png)

      - 损失函数

    - #### 蒙特卡洛方法

      - important sample

        - 重要性采样<https://blog.csdn.net/wangpeng138375/article/details/74645637>

          ![img](https://s2.ax1x.com/2019/02/24/k4T71s.png)

        - 在强化学习方面的应用

          ![img](https://s2.ax1x.com/2019/02/24/k4THcn.png)

          - ordinary importance sampling

            ![img](https://s2.ax1x.com/2019/02/24/k4TbXq.png)

            - unbiased & variance is in general unbounded

          - weighted importance sampling(use weighted average)

            ![img](https://s2.ax1x.com/2019/02/24/k4TvAU.png)

            - biased & usually has dramatically lower variance and is strongly preferred

      - first-visit

      - every-visit

      - 使用了weighted importance sampling 的 off-policy 蒙特卡洛

        ![img](https://s2.ax1x.com/2019/02/24/k4TxNF.png)

      - 对应代码<https://blog.csdn.net/u010223750/article/details/78884172>

      - 优缺点

        - 可以从交互中直接学习优化策略而不需要一个环境的动态模型
        - 缺点是需要大量探索，基于概率，是不确定的

    - #### TD

      - TD偏差

        ![img](https://s2.ax1x.com/2019/02/24/k47Fnx.png)

      - Q-learning

        ![img](https://s2.ax1x.com/2019/02/24/k47VAO.md.png)

      - SARSA

        ![img](https://s2.ax1x.com/2019/02/24/k47ZND.png)

      - n-step sarsa

        ![img](https://s2.ax1x.com/2019/02/24/k47e4e.png)

      - 公式

        ![img](https://s2.ax1x.com/2019/02/24/k47KgA.md.png)

    - #### TD，蒙特卡罗，DP方法的区别

      - DP VS 蒙特卡洛
        - DP方法是需要知道所有可能的状态转移的概率分布，而蒙特卡洛基于抽样数据（前者model-based,后者model-free，是否model-based看它的G是否是公式算出来的）,蒙特卡洛只适用于有限步骤的情节性任务
      - TD VS 蒙特卡洛
        - 蒙特卡洛需要在情节结束后再来估计状态价值
        - TD在行动一步或者几步后根据新状态的价值来估计当前状态价值
        - 蒙特卡洛的期望就是值函数的定义，所以无偏，但是得到的G_t随机性大，方差无穷大
        - TD目标为R_{t+1}+gammaV(S_{t+1}),所以，根据V(S_{t+1})是否为估计值确定是否无偏，但他只用到了有限步骤的随机状态和动作，所以随机性更小，方差小
        - <https://zhuanlan.zhihu.com/p/25913410>
      - TD VS DP
        - 蒙特卡罗的方法使用的是值函数最原始的定义，该方法利用所有回报的累积和估计值函数。DP方法和TD方法则利用一步预测方法计算当前状态值函数。其共同点是利用了bootstrapping方法，不同的是，DP方法利用模型计算后继状态，而TD方法利用试验得到后继状态。

    - Q-learning 和 SARSA 的区别

      - 强化学习中sarsa算法是不是比q-learning算法收敛速度更慢？ - 余帅的回答 - 知乎 <https://www.zhihu.com/question/268461866/answer/355932427>
      - Q-learning是off-policy，sarsa是on-policy，expected sarsa因为使用了期望而非最后真正实行的结果，所以是off-line，但是他的方差比sarsa小
      - sarsa收敛慢，在收敛后依旧会有探索步骤
      - Sarsa选取的是一种保守的策略，他在更新Q值的时候已经为未来规划好了动作，对错误和死亡比较敏感。而Q-learning每次在更新的时候选取的是最大化Q的方向，而当下一个状态时，再重新选择动作，Q-learning是一种鲁莽、大胆、贪婪的算法，对于死亡和错误并不在乎。

    - #### Dyna

      - 算法描述

        ![img](https://s2.ax1x.com/2019/02/24/k47MjI.md.png)

      - 存在这些情况

        - 1.环境随机，而仅仅部分采样可以被观察到
        - 2.模型使用function approximation学习，可能产生的不正确
        - 3.环境变了，而有新的行为没有被观察到

    - #### Dyna+

      - 出现的原因：针对Dyna环境变了，而有新的行为没有被观察到的问题，Dyna+存储了station-action对 的上一次try的time steps，在sample experience from the model 时，将reward 改为 reward + k * 根号（time steps差），这会在update时产生影响
      - 对unstationary的环境适应的更快

    - #### prioritized sweeping

      - 属于一种性能的优化

      - 算法描述

        ![img](https://s2.ax1x.com/2019/02/24/k473Hf.md.png)

  - ### 函数近似

    - why 函数逼近，以及他的好处

      - 第一，对于复杂环境，状态和行动太多，需要大量的空间来记忆策略价值，同时也需要更多的数据和计算量
      - 第二，在许多目标任务中，遇到的状态可能从未出现过
      - 因此需要用一个通用的方法来计算策略价值

    - #### Policy Gradient Methods 

      - 和基于价值函数的区别

        - 选择策略的方式的区别

          - 基于价值函数时，π(s) = argmax_a vπ( s' | s , a ),或者，π(s) = argmax_a qπ(s,a)
          - 基于策略梯度方法时，π(s) = argmax_a π(a | s,θ)

        - 更新规则区别

          - value based

            - 

              ![img](https://s2.ax1x.com/2019/02/24/k47NCQ.md.png)

          - policy based

            - 如果ddiscounting系数γ 为1 ，那么公式如下，不然Gt前面还有内容

              ![img](https://s2.ax1x.com/2019/02/24/k47ags.md.png)

        - 优缺点

          - value-based对于连续动作 不能对每个动作都给予一个Q值 因而在连续动作集合中就不能有很好的表现；policy-based可以有效地处理连续动作集的问题「值函数方法无法确定一个对应max Q的action」，在高维和连续动作空间更加有效，可以学习随机的策略，但同样的容易收敛到局部最小值、方差较大。
          - 如果不用Import Sample,policy gradient 每次更新完theta就要重新采样，因为更新完参数，他的policy分布发生变化，而他更新参数使用的数据是按照policy概率分布采样，既然分布变了，原来的数据就不能用了——on-policy

      - 推导过程

        - The distribution μ here (as in Chapters 9 and 10) is the on-policy distribution under π (see page 199).  J(θ)表示 一个策略所能得到的折扣的奖赏，从初始状态 s0 出发得到的所有的平均

          ![img](https://s2.ax1x.com/2019/02/24/k47dvn.png)

          ![img](https://s2.ax1x.com/2019/02/24/k47yUU.png)

      - with Baseline

        - 新的算法

          - 性能函数J(θ)

            ![img](https://s2.ax1x.com/2019/02/24/k4765F.png)

          - 更新公式

            ![img](https://s2.ax1x.com/2019/02/24/k4775D.png)

          - 具体算法过程

            - 

              ![img](https://s2.ax1x.com/2019/02/24/k47bPe.png)

        - 好处

          - 减少方差

        - baseline 要求

          - 任意函数，甚至是随机变量，但要求与动作无关，这样增加项的和为0

            ![img](https://s2.ax1x.com/2019/02/24/k47L2d.png)

      - Actor-Critic Methods

      - 针对连续性任务

        - 性能函数变为每个步骤的平均奖赏

          ![img](https://s2.ax1x.com/2019/02/24/k4HMiF.png)

      - 无限个动作的连续空间下，策略参数化

        - 不学习每个动作概率，而是学习采取动作的概率分布——设定为分布在实数标量值得动作上的高斯分布，且均值和标准差依赖于动作和给的参数

          ![img](https://s2.ax1x.com/2019/02/24/k4HlRJ.png)

      - 参考资料

        - <https://www.cnblogs.com/steven-yang/p/6624253.html>
        - <https://blog.csdn.net/qq_25037903/article/details/82777802>

- ### 深度强化学习

  - #### value-based
    - DQN
    - double DQN
    - Prioritized Experience Replay
    - Dueling Network Architectures for Deep Reinforcement Learning
    - Distributional value function
    - Noisy DQN
    - RUDDER: Return Decomposition for Delayed Rewards
  - #### policy-based
    - Actor Critic
    - DDPG
    - A3C
    - DDPO
  - #### 信赖域
    - PPO
    - TRPO
    - <https://blog.csdn.net/weixin_41679411/article/details/82421121>

- ### 强化学习面试实战

  - 写Bellman方程
  - sarsa和qlearning区别 哪个谨慎 为什么
  - 介绍PPO，TRPO
  - DDPG中第二个D为什么要确定
  - A3C中的优势函数的意义
  - 如果问题不满足马尔科夫性怎么办，当前时刻的状态和它之前很多状态有关
    - 多个时刻状态并入考虑作为一个状态，或者使用RNN来学习其中隐含的时序信息；
  - DQN怎么设计 动作状态奖励 结构 loss  —— 以flip bird为例设计